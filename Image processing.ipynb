{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eefd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "video_capture_0=cv2.VideoCapture(0)\n",
    "video_capture_1=cv2.VideoCapture(1)\n",
    "while True:\n",
    "    ret0,frame0 =video_capture_0.read()\n",
    "    ret1,frame1=video_capture_1.read()\n",
    "    MIN_MATCH_COUNT = 10\n",
    "\n",
    "    img1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    img2 = cv2.cvtColor(frame0, cv2.COLOR_BGR2GRAY)\n",
    "# Initiate SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "    if len(good)>MIN_MATCH_COUNT:\n",
    "        src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "        matchesMask = mask.ravel().tolist()\n",
    "\n",
    "        h,w = img1.shape\n",
    "        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "        dst = cv2.perspectiveTransform(pts,M)\n",
    "\n",
    "        img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "\n",
    "    else:\n",
    "        print(\"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT))\n",
    "        matchesMask = None\n",
    "    warped_image = cv2.warpPerspective(img1, M, (w, h))\n",
    "    warped_image = cv2.cvtColor(warped_image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# Blend the warped image with image2\n",
    "    blended_image = cv2.addWeighted(frame1, 0.6, warped_image, 0.4,0)\n",
    "    blended_image = cv2.resize(blended_image,(1000,720))\n",
    "    cv2.imshow('res',blended_image)\n",
    "   \n",
    "    if cv2.waitKey(2) & 0xff == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "video_capture_0.release()\n",
    "video_capture_1.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b902801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import imutils\n",
    "\n",
    "img_rgb = cv2.imread(\"suspension-insulator-.jpg\")\n",
    "img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "template = cv2.imread(r\"C:\\Users\\adity\\Downloads\\WhatsApp Image 2023-05-30 at 5.02.51 PM.jpeg\",0)\n",
    "# cv2.imshow('img',template)\n",
    "\n",
    "cv2.imwrite('resized.jpg',template)\n",
    "template = cv2.resize(template,(1000,1000))\n",
    "# for scale in np.linspace(0.025, 0.1, 40):\n",
    "resized_template = imutils.resize(template, width = 400)\n",
    "    \n",
    "res = cv2.matchTemplate(img_gray, resized_template, cv2.TM_SQDIFF)\n",
    "min_val, _, min_loc, _ = cv2.minMaxLoc(res)\n",
    "# if best_match is None or min_val <= best_match[0]:\n",
    "# ideal_scale=scale \n",
    "h, w = resized_template.shape[::]\n",
    "best_match = [min_val, min_loc]\n",
    "        \n",
    "        \n",
    "# print(\"Ideal template image size is : \",ideal_scale)\n",
    "top_left = best_match[1]  \n",
    "bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "cv2.rectangle(img_rgb, top_left, bottom_right, (0, 0,255), 2) \n",
    "cv2.imwrite('matched_resized3.jpg', img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e2d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('img',template)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc475e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import pixellib\n",
    "from pixellib.tune_bg import alter_bg\n",
    "\n",
    "change_bg = alter_bg(model_type = \"pb\")\n",
    "change_bg.load_pascalvoc_model(\"xception_pascalvoc.pb\")\n",
    "change_bg.blur_bg(\"image.jpg\", extreme = True, detect = \"person\", output_image_name=\"blur_img.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "MIN_MATCH_COUNT = 30\n",
    "image1 = cv2.imread(r'C:\\Users\\adity\\Downloads\\img2.jpeg')\n",
    "image2 = cv2.imread(r'C:\\Users\\adity\\Downloads\\img1.jpeg')\n",
    "image2 = cv2.resize(image2,(500,500))\n",
    "image1 = cv2.resize(image1,(500,500))\n",
    "\n",
    "img1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "img2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image1',image1)\n",
    "cv2.imshow('image2',image2)\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "if len(good)>MIN_MATCH_COUNT:\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "\n",
    "    h,w = img1.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "\n",
    "    img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "\n",
    "else:\n",
    "    print(\"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT))\n",
    "    matchesMask = None\n",
    "warped_image = cv2.warpPerspective(img1, M, (w, h))\n",
    "\n",
    "\n",
    "warped_image = cv2.cvtColor(warped_image, cv2.COLOR_GRAY2RGB)\n",
    "warped_image = cv2.resize(warped_image,(image2.shape[1],image2.shape[0]))\n",
    "# Blend the warped image with image2\n",
    "blended_image = cv2.addWeighted(image2, 0.9, warped_image, 0.1,0)\n",
    "# blended_image = cv2.resize(blended_image,(1000,720))\n",
    "# Display the result\n",
    "cv2.imshow('Blended Image', blended_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "#                    singlePointColor = None,\n",
    "#                    matchesMask = matchesMask, # draw only inliers\n",
    "#                    flags = 2)\n",
    "# img3 = cv2.add(img2,img1)\n",
    "# img3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)\n",
    "\n",
    "# plt.imshow(img3,'gray'),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e41071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image2 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\signcropped.jpeg\")\n",
    "cv2.imshow('image2',image2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = img1.shape\n",
    "b = img2.shape\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b074302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy\n",
    "\n",
    "imag = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img2.jpeg\")\n",
    "\n",
    "# coordinates of the pixel\n",
    "X, Y = 0, 0\n",
    "# Get RGB\n",
    "pixelRGB = imag.getpixel((X, Y))\n",
    "R, G, B = pixelRGB\n",
    "brightness = sum([R, G, B]) / 3  ##0 is dark (black) and 255 is bright (white)\n",
    "print(brightness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d727df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install getpixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23602336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "# Load the image\n",
    "# image = Image.open(\"path/to/your/image.jpg\")\n",
    "image = Image.open(r\"C:\\Users\\adity\\Downloads\\img2.jpeg\")\n",
    "# Adjust brightness\n",
    "enhancer = ImageEnhance.Brightness(image)\n",
    "brightened_image = enhancer.enhance(.67)  # Increase brightness by a factor of 1.5\n",
    "\n",
    "# Adjust contrast\n",
    "enhancer = ImageEnhance.Contrast(brightened_image)\n",
    "final_image = enhancer.enhance(1.15)  # Increase contrast by a factor of 1.2\n",
    "\n",
    "# Save the adjusted image\n",
    "final_image.save('img5.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(r\"img5.jpg\")\n",
    "\n",
    "# Convert the image to grayscale\n",
    "grayscale_image = image.convert(\"L\")\n",
    "\n",
    "# Convert the grayscale image to a NumPy array\n",
    "image_array = np.array(grayscale_image)\n",
    "\n",
    "# Calculate the average pixel value (brightness)\n",
    "brightness = np.mean(image_array)\n",
    "\n",
    "# Print the brightness value\n",
    "print(\"Brightness:\", brightness)\n",
    "\n",
    "std_dev = np.std(image_array)\n",
    "\n",
    "# Calculate the contrast as the standard deviation divided by the mean\n",
    "contrast = std_dev / brightness\n",
    "\n",
    "# Print the contrast value\n",
    "print(\"Contrast:\", contrast)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d67fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(r\"C:\\Users\\adity\\Downloads\\img1.jpeg\")\n",
    "\n",
    "# Convert the image to grayscale\n",
    "grayscale_image = image.convert(\"L\")\n",
    "\n",
    "# Convert the grayscale image to a NumPy array\n",
    "image_array = np.array(grayscale_image)\n",
    "\n",
    "# Calculate the average pixel value (brightness)\n",
    "brightness = np.mean(image_array)\n",
    "\n",
    "# Print the brightness value\n",
    "print(\"Brightness:\", brightness)\n",
    "\n",
    "std_dev = np.std(image_array)\n",
    "\n",
    "# Calculate the contrast as the standard deviation divided by the mean\n",
    "contrast = std_dev / brightness\n",
    "\n",
    "# Print the contrast value\n",
    "print(\"Contrast:\", contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ad657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the two images\n",
    "image1 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img2.jpeg\")\n",
    "image2 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img1.jpeg\")\n",
    "\n",
    "# Preprocess the images (resize, convert to grayscale, etc.)\n",
    "image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Feature extraction (SIFT)\n",
    "sift = cv2.SIFT_create()\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(image1_gray, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(image2_gray, None)\n",
    "\n",
    "# Feature matching (Brute-force)\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "# Sort matches by distance\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# Select top n matches\n",
    "n = 50\n",
    "matches = matches[:n]\n",
    "\n",
    "# Extract matching keypoints\n",
    "points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Compute homography matrix\n",
    "M, mask = cv2.findHomography(points1, points2, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Warp image1 to align with image2\n",
    "result_size = (image2.shape[1], image2.shape[0])\n",
    "aligned_image1 = cv2.warpPerspective(image1, M, result_size)\n",
    "\n",
    "# Create a mask for the common portion\n",
    "mask = np.zeros_like(image2_gray)\n",
    "cv2.fillPoly(mask, [np.int32(points2)], (255, 255, 255))\n",
    "\n",
    "# Convert the mask to 3-channel\n",
    "mask_3ch = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "mask_3ch = cv2.normalize(mask_3ch, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "# Apply the mask to the aligned image\n",
    "masked_image1 = cv2.bitwise_and(aligned_image1, mask_3ch)\n",
    "masked_image1 = cv2.normalize(masked_image1, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "# Perform the superimposition\n",
    "result = cv2.addWeighted(image2, 0.9, masked_image1, 0.1, 0)\n",
    "cv2.imwrite('superimpose.jpg',result)\n",
    "# Visualize the superimposed image\n",
    "cv2.imshow('Result', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd5f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ce6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "im1 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img1.jpeg\")          # Image that needs to be registered.\n",
    "im2 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img2.jpeg\") # trainImage\n",
    "\n",
    "img1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "img2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create(100)  #Registration works with at least 50 points\n",
    "\n",
    "# find the keypoints and descriptors with orb\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)  #kp1 --> list of keypoints\n",
    "kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "#Brute-Force matcher takes the descriptor of one feature in first set and is \n",
    "#matched with all other features in second set using some distance calculation.\n",
    "# create Matcher object\n",
    "\n",
    "matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "\n",
    "# Match descriptors.\n",
    "matches = matcher.match(des1, des2, None)  #Creates a list of all matches, just like keypoints\n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "#Like we used cv2.drawKeypoints() to draw keypoints, \n",
    "#cv2.drawMatches() helps us to draw the matches. \n",
    "#https://docs.opencv.org/3.0-beta/modules/features2d/doc/drawing_function_of_keypoints_and_matches.html\n",
    "# Draw first 10 matches.\n",
    "img3 = cv2.drawMatches(im1,kp1, im2, kp2, matches[:10], None)\n",
    "\n",
    "cv2.imshow(\"Matches image\", img3)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#Now let us use these key points to register two images. \n",
    "#Can be used for distortion correction or alignment\n",
    "#For this task we will use homography. \n",
    "# https://docs.opencv.org/3.4.1/d9/dab/tutorial_homography.html\n",
    "\n",
    "# Extract location of good matches.\n",
    "# For this we will use RANSAC.\n",
    "#RANSAC is abbreviation of RANdom SAmple Consensus, \n",
    "#in summary it can be considered as outlier rejection method for keypoints.\n",
    "#http://eric-yuan.me/ransac/\n",
    "#RANSAC needs all key points indexed, first set indexed to queryIdx\n",
    "#Second set to #trainIdx. \n",
    "\n",
    "points1 = np.zeros((len(matches), 2), dtype=np.float32)  #Prints empty array of size equal to (matches, 2)\n",
    "points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "for i, match in enumerate(matches):\n",
    "   points1[i, :] = kp1[match.queryIdx].pt    #gives index of the descriptor in the list of query descriptors\n",
    "   points2[i, :] = kp2[match.trainIdx].pt    #gives index of the descriptor in the list of train descriptors\n",
    "\n",
    "#Now we have all good keypoints so we are ready for homography.   \n",
    "# Find homography\n",
    "#https://en.wikipedia.org/wiki/Homography_(computer_vision)\n",
    "  \n",
    "h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    " \n",
    "  # Use homography\n",
    "height, width, channels = im2.shape\n",
    "im1Reg = cv2.warpPerspective(im1, h, (width, height))  #Applies a perspective transformation to an image.\n",
    "\n",
    "cv2.imshow(\"Registered image\", im1Reg)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff95e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the two images\n",
    "image1 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img1.jpeg\")\n",
    "image2 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img2.jpeg\")\n",
    "\n",
    "# Perform histogram matching to make the images visually consistent\n",
    "matched_image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2Lab)\n",
    "matched_image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2Lab)\n",
    "matched_image2[:, :, 0] = cv2.normalize(matched_image2[:, :, 0], matched_image1[:, :, 0].min(), matched_image1[:, :, 0].max(), norm_type=cv2.NORM_MINMAX)\n",
    "matched_image2 = cv2.cvtColor(matched_image2, cv2.COLOR_Lab2BGR)\n",
    "\n",
    "# Preprocess the images (resize, convert to grayscale, etc.)\n",
    "image1_gray = cv2.cvtColor(matched_image1, cv2.COLOR_BGR2GRAY)\n",
    "image2_gray = cv2.cvtColor(matched_image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Feature extraction (SIFT)\n",
    "sift = cv2.SIFT_create()\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(image1_gray, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(image2_gray, None)\n",
    "\n",
    "# Feature matching (Brute-force)\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "# Sort matches by distance\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# Select top n matches\n",
    "n = 50\n",
    "matches = matches[:n]\n",
    "\n",
    "# Extract matching keypoints\n",
    "points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Compute homography matrix\n",
    "M, mask = cv2.findHomography(points1, points2, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Warp image1 to align with image2\n",
    "result_size = (image2.shape[1], image2.shape[0])\n",
    "aligned_image1 = cv2.warpPerspective(matched_image1, M, result_size)\n",
    "\n",
    "# Create a mask for the common portion\n",
    "mask = np.zeros_like(image2_gray)\n",
    "cv2.fillPoly(mask, [np.int32(points2)], (255, 255, 255))\n",
    "\n",
    "# Convert the mask to 3-channel\n",
    "mask_3ch = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# Apply the mask to the aligned image\n",
    "masked_image1 = cv2.bitwise_and(aligned_image1, mask_3ch)\n",
    "\n",
    "# Perform the superimposition\n",
    "result = cv2.addWeighted(matched_image2, 1, masked_image1, 1, 0)\n",
    "\n",
    "# Visualize the superimposed image\n",
    "cv2.imshow('Result', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1324e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the UV and visible light images\n",
    "uv_image = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img2.jpeg\")\n",
    "visible_image = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img1.jpeg\")\n",
    "# Resize the images to ensure they have the same dimensions\n",
    "uv_image = cv2.resize(uv_image,None,fx=10,fy=10,interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Convert the UV image to grayscale\n",
    "uv_gray = cv2.cvtColor(uv_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Normalize the UV image\n",
    "uv_normalized = cv2.normalize(uv_gray, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "# Create a 3-channel UV image\n",
    "uv_image_rgb = cv2.cvtColor(uv_normalized, cv2.COLOR_GRAY2BGR)\n",
    "visible_image = cv2.resize(visible_image,(uv_image_rgb.shape[1],uv_image_rgb.shape[0]))\n",
    "# Overlay the UV image onto the visible light image\n",
    "overlay = cv2.addWeighted(visible_image, 0.7, uv_image_rgb, 0.3, 0)\n",
    "overlay = cv2.resize(overlay,(1280,720),interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Display the overlaid image\n",
    "cv2.imshow('Overlay', overlay)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: UV Image Analysis\n",
    "uv_image = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img2.jpeg\", 0)  # Read the UV image in grayscale\n",
    "\n",
    "# Preprocessing\n",
    "uv_image = cv2.medianBlur(uv_image, 1)  # Apply median blur for noise reduction\n",
    "\n",
    "# Thresholding\n",
    "_, threshold = cv2.threshold(uv_image, 100, 255, cv2.THRESH_BINARY)  # Adjust the threshold value as needed\n",
    "\n",
    "# Contour detection\n",
    "contours, _ = cv2.findContours(threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Step 2: Visible Light Image Localization\n",
    "visible_image = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img1.jpeg\")\n",
    "\n",
    "# Convert the visible image to grayscale\n",
    "gray_visible = cv2.cvtColor(visible_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a mask of the UV regions by drawing the contours on a black background\n",
    "mask = np.zeros_like(gray_visible)\n",
    "cv2.drawContours(mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "# Apply the mask to the visible image to extract the UV regions\n",
    "uv_regions = cv2.bitwise_and(visible_image, visible_image, mask=mask)\n",
    "\n",
    "# Create a superimposed image by adding the UV regions to the visible image\n",
    "superimposed_image = cv2.addWeighted(visible_image, 0.7, uv_regions, 0.3, 3)\n",
    "#\n",
    "# Display the superimposed image\n",
    "cv2.imshow('Superimposed Image', uv_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcab174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the two images\n",
    "image1 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img1.jpeg\")\n",
    "image2 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img2.jpeg\")\n",
    "\n",
    "# Adjust the zoom level of image1\n",
    "resized_image1 = cv2.resize(image1, None, fx=0.3, fy=0.3)  # Adjust the zoom level as desired\n",
    "\n",
    "# Get the dimensions of image2\n",
    "height, width, _ = image1.shape\n",
    "\n",
    "# Create a new blank image with the dimensions of image2\n",
    "output_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "# Calculate the translation offset for image2\n",
    "tx = 90  # Translate along the x-axis (horizontal)\n",
    "ty = 40  # Translate along the y-axis (vertical)\n",
    "\n",
    "# Paste image1 onto the blank image\n",
    "output_image[:resized_image1.shape[0], :resized_image1.shape[1]] = resized_image1\n",
    "\n",
    "# Superimpose image2 on top of image1 with translation\n",
    "M = np.float32([[1, 0, tx], [0, 1, ty]])  # Translation matrix\n",
    "translated_image2 = cv2.warpAffine(resized_image1, M, (width, height))  # Apply translation to image2\n",
    "blended_image = cv2.addWeighted(output_image, 0.7, translated_image2, 0.3, 0)  # Adjust the blending ratio as desired\n",
    "# blended_image=cv2.cvtColor(blended_image,cv2.COLOR_GRAY2BGR)\n",
    "# Display the resulting image\n",
    "cv2.imshow('Superimposed Image', blended_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the resulting image\n",
    "cv2.imwrite('superimposed_image.jpg', blended_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14502f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the two images\n",
    "image1 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img2.jpeg\")\n",
    "image2 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img1.jpeg\")\n",
    "\n",
    "# Adjust the zoom level of image1\n",
    "zoom_factor = 0.3  # Adjust the zoom level as desired\n",
    "zoomed_image1 = cv2.resize(image1, None, fx=zoom_factor, fy=zoom_factor)\n",
    "\n",
    "# Calculate the translation offsets for the zoomed image\n",
    "tx = 260  # Translate along the x-axis (horizontal)\n",
    "ty = 300  # Translate along the y-axis (vertical)\n",
    "\n",
    "# Create a translation matrix for the zoomed image\n",
    "M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "\n",
    "# Apply translation to the zoomed image\n",
    "translated_zoomed_image = cv2.warpAffine(zoomed_image1, M, (image2.shape[1], image2.shape[0]))\n",
    "\n",
    "# Equalize the histograms of the color channels separately\n",
    "equalized_image1 = cv2.merge([cv2.equalizeHist(image2[:,:,0]), cv2.equalizeHist(image2[:,:,1]), cv2.equalizeHist(image2[:,:,2])])\n",
    "equalized_translated_zoomed_image = cv2.merge([cv2.equalizeHist(translated_zoomed_image[:,:,0]), cv2.equalizeHist(translated_zoomed_image[:,:,1]), cv2.equalizeHist(translated_zoomed_image[:,:,2])])\n",
    "\n",
    "# Superimpose the equalized images\n",
    "superimposed_image = cv2.addWeighted(equalized_image1, 0.85, equalized_translated_zoomed_image, 0.15, 0)  # Adjust the blending ratio as desired\n",
    "\n",
    "# Equalize the histograms of the color channels in the superimposed image\n",
    "equalized_superimposed_image = cv2.merge([cv2.equalizeHist(superimposed_image[:, :, 0]), cv2.equalizeHist(superimposed_image[:, :, 1]), cv2.equalizeHist(superimposed_image[:, :, 2])])\n",
    "\n",
    "# Display the resulting image\n",
    "cv2.imshow('Superimposed Image', equalized_superimposed_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite('superimposed_image2.jpg', equalized_superimposed_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Simultaneous UV and visible image capture\n",
    "uv_camera = cv2.VideoCapture(0)  # UV camera\n",
    "visible_camera = cv2.VideoCapture(1)  # Visible camera\n",
    "\n",
    "# Load camera calibration parameters\n",
    "# These parameters can be obtained by calibrating the cameras using a calibration target\n",
    "# Adjust the file paths according to your calibration result\n",
    "calibration_file_uv = \"uv_camera_calibration.xml\"\n",
    "calibration_file_visible = \"visible_camera_calibration.xml\"\n",
    "calibration_data_uv = cv2.FileStorage(calibration_file_uv, cv2.FILE_STORAGE_READ)\n",
    "calibration_data_visible = cv2.FileStorage(calibration_file_visible, cv2.FILE_STORAGE_READ)\n",
    "camera_matrix_uv = calibration_data_uv.getNode(\"camera_matrix\").mat()\n",
    "dist_coeffs_uv = calibration_data_uv.getNode(\"distortion_coefficients\").mat()\n",
    "camera_matrix_visible = calibration_data_visible.getNode(\"camera_matrix\").mat()\n",
    "dist_coeffs_visible = calibration_data_visible.getNode(\"distortion_coefficients\").mat()\n",
    "calibration_data_uv.release()\n",
    "calibration_data_visible.release()\n",
    "\n",
    "# Create a feature detector\n",
    "detector = cv2.ORB_create()\n",
    "\n",
    "# Create a feature matcher\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Initialize variables for image registration\n",
    "prev_kp_uv, prev_des_uv = None, None\n",
    "\n",
    "while True:\n",
    "    # Step 2: Preprocessing\n",
    "    ret_uv, frame_uv = uv_camera.read()\n",
    "    ret_visible, frame_visible = visible_camera.read()\n",
    "\n",
    "    # Perform preprocessing on frame_uv and frame_visible if needed\n",
    "    preprocessed_uv = cv2.cvtColor(frame_uv, cv2.COLOR_BGR2GRAY)\n",
    "    preprocessed_visible = cv2.cvtColor(frame_visible, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Undistort the UV and visible images\n",
    "    preprocessed_uv = cv2.undistort(preprocessed_uv, camera_matrix_uv, dist_coeffs_uv)\n",
    "    preprocessed_visible = cv2.undistort(preprocessed_visible, camera_matrix_visible, dist_coeffs_visible)\n",
    "\n",
    "    # Step 3: Image registration\n",
    "    # Detect keypoints and compute descriptors for the UV image\n",
    "    kp_uv, des_uv = detector.detectAndCompute(preprocessed_uv, None)\n",
    "\n",
    "    if prev_kp_uv is not None and prev_des_uv is not None:\n",
    "        # Detect keypoints and compute descriptors for the visible image\n",
    "        kp_visible, des_visible = detector.detectAndCompute(preprocessed_visible, None)\n",
    "\n",
    "        # Match keypoints between UV and visible images\n",
    "        matches = matcher.match(prev_des_uv, des_visible)\n",
    "\n",
    "        # Filter out good matches using a distance threshold\n",
    "        good_matches = [match for match in matches if match.distance < 100]\n",
    "\n",
    "        # Extract matched keypoints from UV and visible images\n",
    "        uv_points = np.float32([prev_kp_uv[match.queryIdx].pt for match in good_matches]).reshape(-1, 1, 2)\n",
    "        visible_points = np.float32([kp_visible[match.trainIdx].pt for match in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Estimate the transformation matrix using RANSAC\n",
    "        M, _ = cv2.findHomography(uv_points, visible_points, cv2.RANSAC, 5.0)\n",
    "\n",
    "        # Warp the UV image to align with the visible image\n",
    "        registered_uv = cv2.warpPerspective(preprocessed_uv, M, (preprocessed_visible.shape[1], preprocessed_visible.shape[0]))\n",
    "\n",
    "        # Display the registered UV image\n",
    "        cv2.imshow(\"Registered UV Image\", registered_uv)\n",
    "\n",
    "    # Store the current keypoints and descriptors for the next iteration\n",
    "    prev_kp_uv, prev_des_uv = kp_uv, des_uv\n",
    "\n",
    "    # Display the preprocessed UV and visible images\n",
    "    cv2.imshow(\"Preprocessed UV Image\", preprocessed_uv)\n",
    "    cv2.imshow(\"Preprocessed Visible Image\", preprocessed_visible)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the cameras and close windows\n",
    "uv_camera.release()\n",
    "visible_camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Simultaneous UV and visible image capture\n",
    "uv_camera = cv2.VideoCapture(0)  # UV camera\n",
    "visible_camera = cv2.VideoCapture(1)  # Visible camera\n",
    "\n",
    "# Create a feature detector\n",
    "detector = cv2.ORB_create()\n",
    "\n",
    "# Create a feature matcher\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Initialize variables for image registration\n",
    "prev_kp_uv, prev_des_uv = None, None\n",
    "\n",
    "while True:\n",
    "    # Step 2: Preprocessing\n",
    "    ret_uv, frame_uv = uv_camera.read()\n",
    "    ret_visible, frame_visible = visible_camera.read()\n",
    "\n",
    "    # Perform preprocessing on frame_uv and frame_visible if needed\n",
    "    preprocessed_uv = cv2.cvtColor(frame_uv, cv2.COLOR_BGR2GRAY)\n",
    "    preprocessed_uv = cv2.GaussianBlur(preprocessed_uv, (3, 3), 0)\n",
    "    preprocessed_uv = cv2.equalizeHist(preprocessed_uv)  # Contrast enhancement\n",
    "\n",
    "    preprocessed_visible = cv2.GaussianBlur(frame_visible, (3, 3), 0)\n",
    "    preprocessed_visible = cv2.cvtColor(preprocessed_visible, cv2.COLOR_BGR2GRAY)\n",
    "    preprocessed_visible = cv2.equalizeHist(preprocessed_visible)  # Contrast enhancement\n",
    "\n",
    "    # Illumination correction using adaptive histogram equalization\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    preprocessed_uv = clahe.apply(preprocessed_uv)\n",
    "    preprocessed_visible = clahe.apply(preprocessed_visible)\n",
    "\n",
    "    # Region of interest extraction\n",
    "    roi_mask = np.zeros_like(preprocessed_uv)\n",
    "    # Define the region of interest coordinates (x, y, width, height) based on your requirements\n",
    "    roi = (100, 100, 300, 300)\n",
    "    roi_mask[roi[1]:roi[1] + roi[3], roi[0]:roi[0] + roi[2]] = 255\n",
    "    preprocessed_uv = cv2.bitwise_and(preprocessed_uv, roi_mask)\n",
    "    preprocessed_visible = cv2.bitwise_and(preprocessed_visible, roi_mask)\n",
    "\n",
    "    # Step 3: Image registration\n",
    "    # Detect keypoints and compute descriptors for the UV image\n",
    "    kp_uv, des_uv = detector.detectAndCompute(preprocessed_uv, None)\n",
    "\n",
    "    if prev_kp_uv is not None and prev_des_uv is not None:\n",
    "        # Detect keypoints and compute descriptors for the visible image\n",
    "        kp_visible, des_visible = detector.detectAndCompute(preprocessed_visible, None)\n",
    "\n",
    "        # Match keypoints between UV and visible images\n",
    "        matches = matcher.match(prev_des_uv, des_visible)\n",
    "\n",
    "        # Filter out good matches using a distance threshold\n",
    "        good_matches = [match for match in matches if match.distance < 100]\n",
    "\n",
    "        # Extract matched keypoints from UV and visible images\n",
    "        uv_points = np.float32([prev_kp_uv[match.queryIdx].pt for match in good_matches]).reshape(-1, 1, 2)\n",
    "        visible_points = np.float32([kp_visible[match.trainIdx].pt for match in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Estimate the transformation matrix using RANSAC\n",
    "        M, _ = cv2.findHomography(uv_points, visible_points, cv2.RANSAC, 5.0)\n",
    "\n",
    "        # Warp the UV image to align with the visible image\n",
    "        registered_uv = cv2.warpPerspective(preprocessed_uv, M, (preprocessed_visible.shape[1], preprocessed_visible.shape[0]))\n",
    "\n",
    "        # Display the registered UV image\n",
    "        cv2.imshow(\"Registered UV Image\", registered_uv)\n",
    "\n",
    "    # Store the current keypoints and descriptors for the next iteration\n",
    "    prev_kp_uv, prev_des_uv = kp_uv, des_uv\n",
    "\n",
    "    # Display the preprocessed UV and visible images\n",
    "    cv2.imshow(\"Preprocessed UV Image\", preprocessed_uv)\n",
    "    cv2.imshow(\"Preprocessed Visible Image\", preprocessed_visible)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the cameras and close windows\n",
    "uv_camera.release()\n",
    "visible_camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Set the number of inner corners in the calibration pattern\n",
    "pattern_size = (9, 6)  # Adjust according to your chessboard pattern\n",
    "\n",
    "# Set the size of the calibration square in millimeters\n",
    "square_size = 25.0  # Adjust according to your chessboard pattern\n",
    "\n",
    "# Set the termination criteria for the calibration process\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# Prepare object points based on the calibration pattern\n",
    "object_points = np.zeros((np.prod(pattern_size), 3), dtype=np.float32)\n",
    "object_points[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2) * square_size\n",
    "\n",
    "# Arrays to store object points and image points from calibration images\n",
    "obj_points = []  # 3D points in real world space\n",
    "img_points = []  # 2D points in image plane\n",
    "\n",
    "# Read calibration images and detect calibration pattern\n",
    "calibration_images = glob.glob('calibration_images/*.jpg')  # Adjust the path to your calibration images\n",
    "for image_file in calibration_images:\n",
    "    image = cv2.imread(image_file)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)\n",
    "\n",
    "    if ret:\n",
    "        obj_points.append(object_points)\n",
    "        corners = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n",
    "        img_points.append(corners)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        image = cv2.drawChessboardCorners(image, pattern_size, corners, ret)\n",
    "        cv2.imshow('Chessboard Corners', image)\n",
    "        cv2.waitKey(500)  # Display the image for a short duration\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Perform camera calibration\n",
    "ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(obj_points, img_points, gray.shape[::-1], None, None)\n",
    "\n",
    "# Save the calibration parameters to an XML file\n",
    "calibration_data = cv2.FileStorage('uv_camera_calibration.xml', cv2.FILE_STORAGE_WRITE)\n",
    "calibration_data.write(\"camera_matrix\", camera_matrix)\n",
    "calibration_data.write(\"distortion_coefficients\", dist_coeffs)\n",
    "calibration_data.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the two images\n",
    "image1 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img1.jpeg\")\n",
    "image2 = cv2.imread(r\"C:\\Users\\adity\\Downloads\\img2.jpeg\")\n",
    "\n",
    "# Perform keypoint detection and feature extraction\n",
    "orb = cv2.ORB_create()\n",
    "kp1, des1 = orb.detectAndCompute(image1, None)\n",
    "kp2, des2 = orb.detectAndCompute(image2, None)\n",
    "\n",
    "# Feature matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "top_matches = matches[:10]\n",
    "\n",
    "# Calculate zoom factor\n",
    "zoom_factor = 0.3\n",
    "\n",
    "if len(top_matches) > 0:\n",
    "    sum_distance = 0.0\n",
    "\n",
    "    for match in top_matches:\n",
    "        sum_distance += match.distance\n",
    "\n",
    "    average_distance = sum_distance / len(top_matches)\n",
    "    scale_factor = 1\n",
    "    zoom_factor = .3+ scale_factor * average_distance\n",
    "\n",
    "# Resize images\n",
    "new_width1 = int(image1.shape[1] * zoom_factor)\n",
    "new_height1 = int(image1.shape[0] * zoom_factor)\n",
    "resized_image1 = cv2.resize(image1, (new_width1, new_height1))\n",
    "\n",
    "new_width2 = int(image2.shape[1] * zoom_factor)\n",
    "new_height2 = int(image2.shape[0] * zoom_factor)\n",
    "resized_image2 = cv2.resize(image2, (new_width2, new_height2))\n",
    "\n",
    "# Display the results\n",
    "cv2.imshow('Image 1', resized_image1)\n",
    "cv2.imshow('Image 2', resized_image2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def calculate_brightness(image_path):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    grayscale_image = image.convert('L')\n",
    "\n",
    "    # Calculate the brightness\n",
    "    histogram = grayscale_image.histogram()\n",
    "    pixels = sum(histogram)\n",
    "    brightness = scale = len(histogram)\n",
    "\n",
    "    for index in range(0, scale):\n",
    "        ratio = histogram[index] / pixels\n",
    "        brightness += ratio * (-scale + index)\n",
    "\n",
    "    # Normalize the brightness value\n",
    "    brightness /= scale\n",
    "\n",
    "    return brightness\n",
    "\n",
    "# Provide the paths to your two images\n",
    "image_path1 = 'new1.jpg'\n",
    "image_path2 = 'new2.jpg'\n",
    "path3 = 'new3.jpg'\n",
    "path4  = 'new4.jpg'\n",
    "\n",
    "# Calculate the brightness of the first image\n",
    "brightness1 = calculate_brightness(image_path1)\n",
    "br3=calculate_brightness(path3)\n",
    "br4=calculate_brightness(path4)\n",
    "\n",
    "# Calculate the brightness of the second image\n",
    "brightness2 = calculate_brightness(image_path2)\n",
    "\n",
    "# Print the brightness values\n",
    "print(\"Brightness level of image 1:\", brightness1)\n",
    "print(\"Brightness level of image 2:\", brightness2)\n",
    "print(br3)\n",
    "print(br4)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3014983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def calculate_intensity(image_path):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    grayscale_image = image.convert('L')\n",
    "\n",
    "    # Get the pixel values\n",
    "    pixels = list(grayscale_image.getdata())\n",
    "\n",
    "    # Calculate the average intensity\n",
    "    intensity = sum(pixels) / len(pixels)\n",
    "\n",
    "    return intensity\n",
    "\n",
    "# Provide the path to your image\n",
    "image_path = 'new1.jpg'\n",
    "\n",
    "img2='new2.jpg'\n",
    "img3='new3.jpg'\n",
    "img4='new4.jpg'\n",
    "\n",
    "# Calculate the intensity level of the image\n",
    "intensity = calculate_intensity(image_path)\n",
    "img2=calculate_intensity(img2)\n",
    "img3=calculate_intensity(img3)\n",
    "img4=calculate_intensity(img4)\n",
    "# Print the intensity level\n",
    "print(\"Intensity level:\", intensity)\n",
    "print(img2,img3,img4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def find_max_intensity_portions(image_path):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    grayscale_image = image.convert('L')\n",
    "\n",
    "    # Get the dimensions of the image\n",
    "    width, height = grayscale_image.size\n",
    "\n",
    "    # Initialize variables for maximum intensity and portions\n",
    "    max_intensity = 0\n",
    "    max_intensity_portions = []\n",
    "\n",
    "    # Iterate over the image pixels\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # Get the intensity of the current pixel\n",
    "            intensity = grayscale_image.getpixel((x, y))\n",
    "\n",
    "            # Check if the intensity is greater than the current maximum\n",
    "            if intensity > max_intensity:\n",
    "                max_intensity = intensity\n",
    "                max_intensity_portions = [(x, y)]\n",
    "            elif intensity == max_intensity:\n",
    "                max_intensity_portions.append((x, y))\n",
    "\n",
    "    return max_intensity_portions\n",
    "\n",
    "# Provide the path to your image\n",
    "image_path = 'test2.jpg'\n",
    "\n",
    "# Find the portions with the maximum intensity\n",
    "max_intensity_portions = find_max_intensity_portions(image_path)\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Create a draw object\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Define the radius of the circles\n",
    "radius = 10\n",
    "\n",
    "# Draw a circle around each portion with the maximum intensity\n",
    "for portion in max_intensity_portions:\n",
    "    x, y = portion\n",
    "    circle_coords = (x - radius, y - radius, x + radius, y + radius)\n",
    "    draw.ellipse(circle_coords, outline='red', width=2)\n",
    "\n",
    "# Save the modified image\n",
    "image.save('out3.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05608ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def find_max_intensity_portions(image_path):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    grayscale_image = image.convert('L')\n",
    "\n",
    "    # Get the dimensions of the image\n",
    "    width, height = grayscale_image.size\n",
    "\n",
    "    # Initialize variables for maximum intensity and portions\n",
    "    max_intensity = 0\n",
    "    max_intensity_portions = []\n",
    "\n",
    "    # Iterate over the image pixels\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # Get the intensity of the current pixel\n",
    "            intensity = grayscale_image.getpixel((x, y))\n",
    "\n",
    "            # Check if the intensity is greater than the current maximum\n",
    "            if intensity > max_intensity:\n",
    "                max_intensity = intensity\n",
    "                max_intensity_portions = [(x, y)]\n",
    "            elif intensity == max_intensity:\n",
    "                max_intensity_portions.append((x, y))\n",
    "\n",
    "    return max_intensity_portions\n",
    "\n",
    "# Provide the path to your image\n",
    "image_path = 'test1.jpg'\n",
    "\n",
    "# Find the portions with the maximum intensity\n",
    "max_intensity_portions = find_max_intensity_portions(image_path)\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Create a blank frame image with the same size as the original image\n",
    "frame_image = Image.new('RGB', image.size, color='white')\n",
    "\n",
    "# Create a draw object for the frame image\n",
    "draw_frame = ImageDraw.Draw(frame_image)\n",
    "\n",
    "# Define the size of the rectangles\n",
    "rect_width = 200\n",
    "rect_height = 200\n",
    "\n",
    "# Draw a rectangle on the frame image for each portion with the maximum intensity\n",
    "for portion in max_intensity_portions:\n",
    "    x, y = portion\n",
    "    x1 = x - (rect_width // 2)\n",
    "    y1 = y - (rect_height // 2)\n",
    "    x2 = x1 + rect_width\n",
    "    y2 = y1 + rect_height\n",
    "    draw_frame.rectangle((x1, y1, x2, y2), outline='red', width=2)\n",
    "\n",
    "# Save the frame image with the rectangles\n",
    "frame_image.save('out3.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c62f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the image\n",
    "image = cv2.imread('test2.jpg')\n",
    "image=cv2.resize(image,(720,720))\n",
    "# Step 2: Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step 3: Calculate pixel intensities\n",
    "intensities = gray.flatten()\n",
    "\n",
    "# Step 4: Find the maximum intensity\n",
    "max_intensity = np.max(intensities)\n",
    "max_index = np.argmax(intensities)\n",
    "\n",
    "# Step 5: Highlight or extract the most intense portion\n",
    "# In this example, we'll draw a rectangle around the portion\n",
    "# with the maximum intensity.\n",
    "height, width = gray.shape\n",
    "max_row = max_index // width\n",
    "max_col = max_index % width\n",
    "\n",
    "rect_size = 80  # Size of the rectangle to highlight the portion\n",
    "top_left = (max_col - rect_size, max_row - rect_size)\n",
    "bottom_right = (max_col + rect_size, max_row + rect_size)\n",
    "cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)\n",
    "# Step 6: Display or save the result\n",
    "cv2.imshow('Most Intense Portion', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ed00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the image\n",
    "image = cv2.imread('test2.jpg')\n",
    "image=cv2.resize(image,(720,720))\n",
    "\n",
    "# Step 2: Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step 3: Calculate pixel intensities\n",
    "intensities = gray.flatten()\n",
    "\n",
    "# Step 4: Find the maximum intensity\n",
    "max_intensity = np.max(intensities)\n",
    "max_indices = np.where(intensities == max_intensity)[0]\n",
    "\n",
    "# Step 5: Extract the most intense portion\n",
    "rows, cols = gray.shape\n",
    "top_row = np.min(max_indices) // cols\n",
    "bottom_row = np.max(max_indices) // cols\n",
    "left_col = np.min(max_indices) % cols\n",
    "right_col = np.max(max_indices) % cols\n",
    "\n",
    "# Crop the portion with the highest intensity\n",
    "portion = image[top_row:bottom_row, left_col:right_col]\n",
    "\n",
    "# Step 6: Draw a rectangle around the portion\n",
    "cv2.rectangle(image, (left_col, top_row), (right_col, bottom_row), (0, 255, 0), 2)\n",
    "\n",
    "# Step 7: Display or save the result\n",
    "cv2.imshow('Most Intense Portion', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the image\n",
    "image = cv2.imread('test5.jpg')\n",
    "image=cv2.resize(image,(720,720))\n",
    "# Step 2: Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step 3: Calculate pixel intensities\n",
    "intensities = gray.flatten()\n",
    "\n",
    "# Step 4: Find the maximum intensity\n",
    "max_intensity = np.max(intensities)\n",
    "max_indices = np.where(intensities == max_intensity)[0]\n",
    "\n",
    "# Step 5: Extract the most intense portion\n",
    "rows, cols = gray.shape\n",
    "top_row = np.min(max_indices) // cols\n",
    "bottom_row = np.max(max_indices) // cols\n",
    "left_col = np.min(max_indices) % cols\n",
    "right_col = np.max(max_indices) % cols\n",
    "\n",
    "# Adjust the portion to include the entire connected component\n",
    "ret, threshold = cv2.threshold(gray, max_intensity - 1, 255, cv2.THRESH_BINARY)\n",
    "contours, _ = cv2.findContours(threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "max_contour = max(contours, key=cv2.contourArea)\n",
    "x, y, width, height = cv2.boundingRect(max_contour)\n",
    "top_row = y\n",
    "bottom_row = y + height\n",
    "left_col = x\n",
    "right_col = x + width\n",
    "\n",
    "# Crop the portion with the highest intensity\n",
    "portion = image[top_row:bottom_row, left_col:right_col]\n",
    "\n",
    "# Step 6: Draw a rectangle around the portion\n",
    "cv2.rectangle(image, (left_col, top_row), (right_col, bottom_row), (0, 255, 0), 2)\n",
    "\n",
    "# Step 7: Display or save the result\n",
    "cv2.imshow('Most Intense Portion', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "# Step 1: Load the image\n",
    "image = cv2.imread('test1.jpg')\n",
    "image=cv2.resize(image,(720,720))\n",
    "# Step 2: Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step 3: Calculate pixel intensities\n",
    "intensities = gray.flatten()\n",
    "\n",
    "# Step 4: Find the maximum intensity\n",
    "max_intensity = np.max(intensities)\n",
    "max_indices = np.where(intensities == max_intensity)[0]\n",
    "\n",
    "# Step 5: Extract the most intense portion\n",
    "rows, cols = gray.shape\n",
    "top_row = np.min(max_indices) // cols\n",
    "bottom_row = np.max(max_indices) // cols\n",
    "left_col = np.min(max_indices) % cols\n",
    "right_col = np.max(max_indices) % cols\n",
    "\n",
    "# Adjust the portion to include the entire connected component\n",
    "ret, threshold = cv2.threshold(gray, max_intensity - 1, 255, cv2.THRESH_BINARY)\n",
    "contours, _ = cv2.findContours(threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "max_contour = max(contours, key=cv2.contourArea)\n",
    "x, y, width, height = cv2.boundingRect(max_contour)\n",
    "top_row = y\n",
    "bottom_row = y + height\n",
    "left_col = x\n",
    "right_col = x + width\n",
    "\n",
    "# Crop the portion with the highest intensity\n",
    "portion = image[top_row:bottom_row, left_col:right_col]\n",
    "\n",
    "# Step 6: Draw a rectangle around the portion\n",
    "cv2.rectangle(image, (left_col, top_row), (right_col, bottom_row), (0, 255, 0), 2)\n",
    "\n",
    "# Step 7: Display or save the result\n",
    "cv2.imshow('Most Intense Portion', image)\n",
    "# image.save('out1.jpg')\n",
    "# image.save('out3.jpg')\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49552321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the image\n",
    "image = cv2.imread('test2.jpg')\n",
    "image=cv2.resize(image,(1000,1000))\n",
    "# Step 2: Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step 3: Calculate pixel intensities\n",
    "intensities = gray.flatten()\n",
    "\n",
    "# Step 4: Find the maximum intensity\n",
    "max_intensity = np.max(intensities)\n",
    "max_indices = np.where(intensities == max_intensity)[0]\n",
    "\n",
    "# Step 5: Extract the most intense portion\n",
    "rows, cols = gray.shape\n",
    "top_row = np.min(max_indices) // cols\n",
    "bottom_row = np.max(max_indices) // cols\n",
    "left_col = np.min(max_indices) % cols\n",
    "right_col = np.max(max_indices) % cols\n",
    "\n",
    "# Crop the portion with the highest intensity\n",
    "portion = image[top_row:bottom_row, left_col:right_col]\n",
    "\n",
    "# Step 6: Apply edge detection to the portion\n",
    "portion_gray = cv2.cvtColor(portion, cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(portion_gray, 1000, 1000)\n",
    "\n",
    "# Step 7: Draw a rectangle around the portion\n",
    "cv2.rectangle(image, (left_col, top_row), (right_col, bottom_row), (0, 255, 0), 2)\n",
    "\n",
    "# Step 8: Display or save the result\n",
    "cv2.imshow('Highest Intensity Portion with Edges', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164225e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
